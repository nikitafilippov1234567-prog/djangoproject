import pandas as pd
import numpy as np
import pickle
import os
import warnings
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.metrics import classification_report, mean_squared_error, r2_score, mean_absolute_error
import shap
import lime
import lime.lime_tabular
import matplotlib.pyplot as plt
import seaborn as sns
from tabulate import tabulate

# XGBoost
import xgboost as xgb

# PyTorch
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

warnings.filterwarnings('ignore')

class ImprovedPricePredictor(nn.Module):
    """–ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å, 2 –≤–µ—Ä—Å–∏—è (–ø–æ—Å–ª–µ–¥–Ω—è—è)"""
    
    def __init__(self, input_size, dropout_rate=0.2):
        super(ImprovedPricePredictor, self).__init__()
        
        self.input_layer = nn.Linear(input_size, 256)
        self.bn1 = nn.BatchNorm1d(256)
        
        self.hidden1 = nn.Linear(256, 128)
        self.bn2 = nn.BatchNorm1d(128)
        
        self.hidden2 = nn.Linear(128, 64)
        self.bn3 = nn.BatchNorm1d(64)
        
        self.hidden3 = nn.Linear(64, 32)
        self.bn4 = nn.BatchNorm1d(32)
        
        self.output_layer = nn.Linear(32, 1)
        
        self.dropout = nn.Dropout(dropout_rate)
        self.leaky_relu = nn.LeakyReLU(0.01)
        
        self.apply(self._init_weights)
    
    def _init_weights(self, module):
        if isinstance(module, nn.Linear):
            nn.init.xavier_uniform_(module.weight)
            if module.bias is not None:
                nn.init.constant_(module.bias, 0)
    
    def forward(self, x):
        x = self.input_layer(x)
        x = self.bn1(x)
        x = self.leaky_relu(x)
        x = self.dropout(x)
        
        x = self.hidden1(x)
        x = self.bn2(x)
        x = self.leaky_relu(x)
        x = self.dropout(x)
        
        x = self.hidden2(x)
        x = self.bn3(x)
        x = self.leaky_relu(x)
        x = self.dropout(x)
        
        x = self.hidden3(x)
        x = self.bn4(x)
        x = self.leaky_relu(x)
        x = self.dropout(x)
        
        x = self.output_layer(x)
        return x

class CustomUnpickler(pickle.Unpickler):
    def find_class(self, module, name):
        if module == "__main__" and name == "ImprovedPricePredictor":
            return ImprovedPricePredictor
        return super().find_class(module, name)

def custom_load(file):
    return CustomUnpickler(file).load()

class EarlyStopping:
    """–†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ (80-100 —ç–ø–æ—Ö –≤ —Å—Ä–µ–¥–Ω–µ–º)"""
    
    def __init__(self, patience=15, min_delta=0, restore_best_weights=True):
        self.patience = patience
        self.min_delta = min_delta
        self.restore_best_weights = restore_best_weights
        self.best_loss = None
        self.counter = 0
        self.best_weights = None
        
    def __call__(self, val_loss, model):
        if self.best_loss is None:
            self.best_loss = val_loss
            self.save_checkpoint(model)
        elif val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            self.save_checkpoint(model)
        else:
            self.counter += 1
            
        if self.counter >= self.patience:
            if self.restore_best_weights and self.best_weights is not None:
                model.load_state_dict(self.best_weights)
            return True
        return False
    
    def save_checkpoint(self, model):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –ª—É—á—à–∏–µ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏"""
        self.best_weights = model.state_dict().copy()

class RealEstateAnalyzer:
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.label_encoders = {}
        self.feature_names = []
        self.price_quantiles_4 = None
        self.price_quantiles_3 = None
        self.geo_bounds = None
        self.models_dir = r'M:\djangoproject\predict\models'
        self.model_metrics = {}
        self.model_errors = {}  # –î–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—à–∏–±–æ–∫ –º–æ–¥–µ–ª–µ–π
        
        # –ò–∑-–∑–∞ –∫—Ä–∏–≤–æ–≥–æ –ø–µ—Ä–µ–Ω–æ—Å–∞ –Ω–∞ —Å–µ—Ä–≤–µ—Ä mkdir
        if not os.path.exists(self.models_dir):
            os.makedirs(self.models_dir)
    def diagnose_models(self):
        """–î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è –º–æ–¥–µ–ª–µ–π –∏ –º–µ—Ç—Ä–∏–∫"""
        print("\n" + "="*80)
        print("üîç –î–ò–ê–ì–ù–û–°–¢–ò–ö–ê –ú–û–î–ï–õ–ï–ô –ò –ú–ï–¢–†–ò–ö")
        print("="*80)
        
        print(f"\nüìä –í—Å–µ–≥–æ –º–µ—Ç—Ä–∏–∫: {len(self.model_metrics)}")
        print(f"ü§ñ –í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π: {len(self.models)}")
        print(f"üìè –í—Å–µ–≥–æ —Å–∫–µ–π–ª–µ—Ä–æ–≤: {len(self.scalers)}")
        
        metrics_keys = set(self.model_metrics.keys())
        models_keys = set(self.models.keys())
        
        print("\nüìä –ú–ï–¢–†–ò–ö–ò (–ø–µ—Ä–≤—ã–µ 10):")
        for i, key in enumerate(sorted(self.model_metrics.keys())[:10], 1):
            metrics = self.model_metrics[key]
            r2 = metrics.get('R¬≤', metrics.get('R2', 'N/A'))
            print(f"  {i}. {key} (R¬≤: {r2})")
        if len(self.model_metrics) > 10:
            print(f"  ... –∏ –µ—â—ë {len(self.model_metrics) - 10}")
        
        print("\nü§ñ –ú–û–î–ï–õ–ò (–ø–µ—Ä–≤—ã–µ 10):")
        for i, key in enumerate(sorted(self.models.keys())[:10], 1):
            model_type = type(self.models[key]).__name__
            print(f"  {i}. {key} ({model_type})")
        if len(self.models) > 10:
            print(f"  ... –∏ –µ—â—ë {len(self.models) - 10}")
        
        print("\n‚ö†Ô∏è –ù–ï–°–û–û–¢–í–ï–¢–°–¢–í–ò–Ø:")
        in_metrics_not_models = metrics_keys - models_keys
        in_models_not_metrics = models_keys - metrics_keys
        
        if in_metrics_not_models:
            print(f"\n‚ùå –ï—Å—Ç—å –º–µ—Ç—Ä–∏–∫–∏, –Ω–æ –ù–ï–¢ –º–æ–¥–µ–ª–µ–π ({len(in_metrics_not_models)}):")
            for key in sorted(list(in_metrics_not_models)[:10]):
                print(f"  ‚úó {key}")
            if len(in_metrics_not_models) > 10:
                print(f"  ... –∏ –µ—â—ë {len(in_metrics_not_models) - 10}")
        
        if in_models_not_metrics:
            print(f"\n‚ö†Ô∏è –ï—Å—Ç—å –º–æ–¥–µ–ª–∏, –Ω–æ –ù–ï–¢ –º–µ—Ç—Ä–∏–∫ ({len(in_models_not_metrics)}):")
            for key in sorted(list(in_models_not_metrics)[:10]):
                print(f"  ! {key}")
            if len(in_models_not_metrics) > 10:
                print(f"  ... –∏ –µ—â—ë {len(in_models_not_metrics) - 10}")
        
        if not in_metrics_not_models and not in_models_not_metrics:
            print("  ‚úÖ –í—Å–µ –∫–ª—é—á–∏ —Å–æ–≤–ø–∞–¥–∞—é—Ç!")
        
        print("\n" + "="*80)
        
        return {
            'metrics_only': list(in_metrics_not_models),
            'models_only': list(in_models_not_metrics),
            'matched': list(metrics_keys & models_keys)
        }
    
    def init_similarity_search(self, df):
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ–±—É—á–∞—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ (–±–µ–∑ —Ü–µ–Ω—ã –¥–ª—è –ø–æ–∏—Å–∫–∞)
        feature_columns = ['apartment type', 'minutes to metro', 'number of rooms', 
                        'area', 'living area', 'kitchen area', 'floor', 
                        'number of floors', 'renovation', 'metro_lat', 'metro_lon']
        
        self.training_data = df[feature_columns + ['price']].copy()
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –≤ —á–∏—Å–ª–æ–≤—ã–µ –¥–ª—è –∫–≤–∞–Ω—Ç–∏–ª–µ–π
        self.training_data_numeric = self.training_data.copy()
        
        # Apartment type: secondary=1, new=0
        self.training_data_numeric['apartment type'] = (
            self.training_data_numeric['apartment type'] == 'secondary'
        ).astype(int)
        
        # Renovation: –æ–ø—Ä–µ–¥–µ–ª—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∏ –∫–æ–¥–∏—Ä—É–µ–º
        renovation_mapping = {}
        unique_renovations = self.training_data_numeric['renovation'].unique()
        for i, renovation in enumerate(unique_renovations):
            renovation_mapping[renovation] = i
        self.training_data_numeric['renovation'] = self.training_data_numeric['renovation'].map(renovation_mapping)
        self.renovation_mapping = renovation_mapping
        
        # –í—ã—á–∏—Å–ª—è–µ–º –∫–≤–∞–Ω—Ç–∏–ª–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–∞ (20 –∫–≤–∞–Ω—Ç–∏–ª–µ–π = 5% –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã)
        numeric_features = ['minutes to metro', 'number of rooms', 'area', 'living area', 
                        'kitchen area', 'floor', 'number of floors', 'metro_lat', 'metro_lon']
        
        self.feature_quantiles = {}
        
        for feature in numeric_features:
            # 20 –∫–≤–∞–Ω—Ç–∏–ª–µ–π (–æ—Ç 10% –¥–æ 90% —Å —à–∞–≥–æ–º 10%)
            quantiles = np.percentile(self.training_data_numeric[feature], 
                                    np.arange(10, 100, 10))
            self.feature_quantiles[feature] = quantiles
        
        # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø—Ä–æ—Å—Ç–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        self.feature_quantiles['apartment type'] = [0, 1]  # new, secondary
        self.feature_quantiles['renovation'] = list(range(len(unique_renovations)))
        
        print(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ø–æ–∏—Å–∫ –ø–æ—Ö–æ–∂–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤ –Ω–∞ {len(self.training_data)} –∑–∞–ø–∏—Å—è—Ö")

    def get_feature_quantile_index(self, value, feature_name):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –∫–≤–∞–Ω—Ç–∏–ª—è –¥–ª—è –∑–Ω–∞—á–µ–Ω–∏—è –ø—Ä–∏–∑–Ω–∞–∫–∞"""
        if feature_name in ['apartment type', 'renovation']:
            # –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
            return int(value)
        
        quantiles = self.feature_quantiles[feature_name]
        # –ù–∞—Ö–æ–¥–∏–º –≤ –∫–∞–∫–æ–π –∫–≤–∞–Ω—Ç–∏–ª—å –ø–æ–ø–∞–¥–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ
        quantile_index = np.searchsorted(quantiles, value, side='right')
        return min(quantile_index, len(quantiles) - 1)

    def find_similar_properties(self, sample_data, top_n=10, min_matches=5, metro_stations=None):
        if self.training_data is None or self.training_data_numeric is None or not self.feature_quantiles:
            print("Error: Training data or feature quantiles not initialized for similarity search")
            return []
        
        sample_numeric = {}
        sample_numeric['apartment type'] = 1 if sample_data.get('apartment_type', '').lower() == 'secondary' else 0
        sample_numeric['renovation'] = self.renovation_mapping.get(sample_data.get('renovation', '').lower(), 0)
        
        numeric_features = ['minutes to metro', 'number of rooms', 'area', 'living area', 
                            'kitchen area', 'floor', 'number of floors', 'metro_lat', 'metro_lon']
        
        for feature in numeric_features:
            sample_numeric[feature] = float(sample_data.get(feature.replace(' ', '_').lower(), 0))
        
        sample_quantile_indices = {}
        for feature in sample_numeric.keys():
            sample_quantile_indices[feature] = self.get_feature_quantile_index(
                sample_numeric[feature], feature
            )
        
        # Calculate three nearest metro stations if metro_stations dictionary is provided
        nearest_stations = []
        if metro_stations and sample_data.get('metro_lat') and sample_data.get('metro_lon'):
            input_lat = float(sample_data['metro_lat'])
            input_lon = float(sample_data['metro_lon'])
            distances = []
            for station, coords in metro_stations.items():
                try:
                    station_lat = float(coords['lat'])
                    station_lon = float(coords['lon'])
                    # Haversine distance (in kilometers)
                    from math import radians, sin, cos, sqrt, atan2
                    R = 6371.0  # Earth's radius in km
                    dlat = radians(station_lat - input_lat)
                    dlon = radians(station_lon - input_lon)
                    a = sin(dlat / 2)**2 + cos(radians(input_lat)) * cos(radians(station_lat)) * sin(dlon / 2)**2
                    c = 2 * atan2(sqrt(a), sqrt(1 - a))
                    distance = R * c
                    distances.append((station, distance))
                except (KeyError, ValueError, TypeError):
                    continue
            distances.sort(key=lambda x: x[1])
            nearest_stations = [station for station, _ in distances[:3]]
            print(f"Nearest metro stations: {nearest_stations}")
        
        similar_objects = []
        feature_names = list(sample_numeric.keys())
        
        for idx, row in self.training_data_numeric.iterrows():
            matches = 0
            matched_features = []
            
            # Check if the property's metro coordinates match one of the nearest stations
            if nearest_stations and metro_stations:
                row_lat = row['metro_lat']
                row_lon = row['metro_lon']
                is_near_station = False
                for station in nearest_stations:
                    try:
                        station_lat = float(metro_stations[station]['lat'])
                        station_lon = float(metro_stations[station]['lon'])
                        dlat = radians(station_lat - row_lat)
                        dlon = radians(station_lon - row_lon)
                        a = sin(dlat / 2)**2 + cos(radians(row_lat)) * cos(radians(station_lat)) * sin(dlon / 2)**2
                        c = 2 * atan2(sqrt(a), sqrt(1 - a))
                        distance = R * c
                        if distance < 1.0:  # Within 1 km of a nearest station
                            is_near_station = True
                            break
                    except (KeyError, ValueError, TypeError):
                        continue
                if not is_near_station:
                    continue
            
            for feature in feature_names:
                row_quantile_idx = self.get_feature_quantile_index(row[feature], feature)
                if row_quantile_idx == sample_quantile_indices[feature]:
                    matches += 1
                    matched_features.append(feature)
            
            if matches >= min_matches:
                original_row = self.training_data.iloc[idx]
                similar_objects.append({
                    'matches': matches,
                    'matched_features': matched_features,
                    'price': float(original_row['price']),
                    'apartment_type': original_row['apartment type'],
                    'minutes_to_metro': float(original_row['minutes to metro']),
                    'number_of_rooms': float(original_row['number of rooms']),
                    'area': float(original_row['area']),
                    'living_area': float(original_row['living area']),
                    'kitchen_area': float(original_row['kitchen area']),
                    'floor': float(original_row['floor']),
                    'number_of_floors': float(original_row['number of floors']),
                    'renovation': original_row['renovation'],
                    'metro_lat': float(original_row['metro_lat']),
                    'metro_lon': float(original_row['metro_lon']),
                    'match_percentage': round((matches / len(feature_names)) * 100, 1),
                    'nearest_metro': nearest_stations[0] if nearest_stations else None
                })
        
        similar_objects.sort(key=lambda x: x['matches'], reverse=True)
        print(f"Found {len(similar_objects)} similar properties (min {min_matches} matches)")
        
        return similar_objects[:top_n]

    def remove_outliers_iqr(self, df, columns=None):
        """–£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –º–µ—Ç–æ–¥–æ–º IQR, –ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –Ω–æ –û–ö"""
        df_clean = df.copy()
        
        if columns is None:
            # –ò—Å–∫–ª—é—á–∏—Ç—å price
            columns = df_clean.select_dtypes(include=[np.number]).columns.tolist()
            if 'price' in columns:
                columns.remove('price')
        
        outlier_indices = set()
        
        for column in columns:
            if column in df_clean.columns:
                Q1 = df_clean[column].quantile(0.25)
                Q3 = df_clean[column].quantile(0.75)
                IQR = Q3 - Q1
                
                # –ì—Ä–∞–Ω–∏—Ü—ã
                lower_bound = Q1 - 1.5 * IQR
                upper_bound = Q3 + 1.5 * IQR
                
                # –ò–Ω–¥–µ–∫—Å—ã –≤—ã–±—Ä–æ—Å–æ–≤
                outliers = df_clean[(df_clean[column] < lower_bound) | 
                                  (df_clean[column] > upper_bound)].index
                outlier_indices.update(outliers)
        
        # –£–¥–∞–ª—è–µ–º –≤—ã–±—Ä–æ—Å—ã
        initial_count = len(df_clean)
        df_clean = df_clean.drop(list(outlier_indices))
        final_count = len(df_clean)
        
        print(f"–£–¥–∞–ª–µ–Ω–æ {initial_count - final_count} –≤—ã–±—Ä–æ—Å–æ–≤ ({(initial_count - final_count)/initial_count*100:.2f}%)")
        
        return df_clean
    
    def remove_price_outliers(self, df):
        """–£–¥–∞–ª–µ–Ω–∏–µ —Ü–µ–Ω–æ–≤—ã—Ö –≤—ã–±—Ä–æ—Å–æ–≤ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        df_clean = df.copy()
        
        # –û—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥: –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        print(f"\nüîç –í remove_price_outliers: –¶–µ–Ω—ã –î–û —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:")
        print(df_clean['price'].describe())
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω: {df_clean['price'].nunique()}")
        print(f"–¢–æ–ø-5 —á–∞—Å—Ç—ã—Ö —Ü–µ–Ω:\n{df_clean['price'].value_counts().head()}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –∏ –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ —Ü–µ–Ω—ã
        initial_count = len(df_clean)
        df_clean = df_clean[df_clean['price'].notna() & (df_clean['price'] > 0)]
        final_count = len(df_clean)
        
        # –û—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥: –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
        print(f"\nüîç –í remove_price_outliers: –¶–µ–Ω—ã –ü–û–°–õ–ï —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏:")
        print(df_clean['price'].describe())
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω: {df_clean['price'].nunique()}")
        print(f"–¢–æ–ø-5 —á–∞—Å—Ç—ã—Ö —Ü–µ–Ω:\n{df_clean['price'].value_counts().head()}")
        print(f"–£–¥–∞–ª–µ–Ω–æ {initial_count - final_count} —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏/–æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–º–∏ —Ü–µ–Ω–∞–º–∏ "
            f"({((initial_count - final_count)/initial_count*100):.2f}%)")
        
        return df_clean
    
    def clean_data(self, df):
        """–û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ü–µ–Ω"""
        df_clean = df.copy()
        
        # –û—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥: –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        print(f"\nüîç –í clean_data: –ò—Å—Ö–æ–¥–Ω—ã–µ —Ü–µ–Ω—ã –≤ df:")
        print(df_clean['price'].describe())
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω: {df_clean['price'].nunique()}")
        print(f"–¢–æ–ø-5 —á–∞—Å—Ç—ã—Ö —Ü–µ–Ω:\n{df_clean['price'].value_counts().head()}")
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤
        initial_count = len(df_clean)
        df_clean = df_clean.dropna()
        print(f"\nüîç –ü–æ—Å–ª–µ dropna: –£–¥–∞–ª–µ–Ω–æ {initial_count - len(df_clean)} —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏")
        print(df_clean['price'].describe())
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω: {df_clean['price'].nunique()}")
        
        # –£–¥–∞–ª–µ–Ω–∏–µ —Ü–µ–Ω–æ–≤—ã—Ö –≤—ã–±—Ä–æ—Å–æ–≤ (–º–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è)
        df_clean = self.remove_price_outliers(df_clean)
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        initial_count = len(df_clean)
        df_clean = df_clean.drop_duplicates()
        print(f"\nüîç –ü–æ—Å–ª–µ drop_duplicates: –£–¥–∞–ª–µ–Ω–æ {initial_count - len(df_clean)} –¥—É–±–ª–∏–∫–∞—Ç–æ–≤")
        print(df_clean['price'].describe())
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω: {df_clean['price'].nunique()}")
        
        # –£–¥–∞–ª–µ–Ω–∏–µ –≤—ã–±—Ä–æ—Å–æ–≤ –ø–æ –¥—Ä—É–≥–∏–º –ø—Ä–∏–∑–Ω–∞–∫–∞–º (–º–µ–Ω–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ)
        columns_to_check = ['area', 'living area', 'kitchen area', 'minutes to metro', 
                        'floor', 'number of floors']
        df_clean = self.remove_outliers_iqr(df_clean, columns=columns_to_check)
        
        # –û—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥: —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        print(f"\nüîç –ü–æ—Å–ª–µ remove_outliers_iqr: –§–∏–Ω–∞–ª—å–Ω—ã–µ —Ü–µ–Ω—ã:")
        print(df_clean['price'].describe())
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω: {df_clean['price'].nunique()}")
        print(f"–¢–æ–ø-5 —á–∞—Å—Ç—ã—Ö —Ü–µ–Ω:\n{df_clean['price'].value_counts().head()}")
        print(f"–ò—Ç–æ–≥–æ –æ—Å—Ç–∞–ª–æ—Å—å {len(df_clean)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {initial_count} "
            f"({len(df_clean)/initial_count*100:.2f}%)")
        
        return df_clean
    
    def preprocess_geodata(self, df):
        """ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –≥–µ–æ–¥–∞–Ω–Ω—ã—Ö - —Å–æ–∑–¥–∞–Ω–∏–µ —Å–∏–Ω—É—Å–∞ –∏ –∫–æ—Å–∏–Ω—É—Å–∞ —É–≥–ª–æ–≤ (–∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ GWR –ø–æ–æ—Å—Ç–∞—Ç–æ–∫–∞–º, –æ—á–µ–Ω—å —Å–ª–æ–∂–Ω–æ, –ª–∏–±–æ geohash, –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç)"""
        if 'metro_lat' not in df.columns or 'metro_lon' not in df.columns:
            print("metro_lat –∏–ª–∏ metro_lon –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç (newdata)")
            # –î–æ–±–∞–≤–ª—è–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
            df['geo_sin_angle'] = 0
            df['geo_cos_angle'] = 1
            return df
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –¥–∞–Ω–Ω—ã—Ö –≤ metro_lat –∏ metro_lon
        if df['metro_lat'].isna().all() or df['metro_lon'].isna().all():
            print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –í—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è metro_lat –∏–ª–∏ metro_lon —è–≤–ª—è—é—Ç—Å—è NaN. –ü—Ä–æ–≤–µ—Ä–∏—Ç—å, –∫–æ–ª–æ–Ω–∫–∏ –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å –º–∞–ª–µ–Ω—å–∫–æ–π –±—É–∫–≤—ã")
            df['geo_sin_angle'] = 0
            df['geo_cos_angle'] = 1
            return df
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å NaN –≤ metro_lat –∏–ª–∏ metro_lon –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏—è –≥—Ä–∞–Ω–∏—Ü (–¥–∞—Ç–∞—Å–µ—Ç –∏—Å–ø—Ä–∞–≤–ª–µ–Ω, —ç—Ç–æ –º—É—Å–æ—Ä)
        geo_data = df[['metro_lat', 'metro_lon']].dropna()
        
        if len(geo_data) == 0:
            print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –≥–µ–æ–¥–∞–Ω–Ω—ã—Ö –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è NaN")
            df['geo_sin_angle'] = 0
            df['geo_cos_angle'] = 1
            return df
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä–∞–Ω–∏—Ü—ã –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç
        if not hasattr(self, 'geo_bounds') or self.geo_bounds is None:
            try:
                self.geo_bounds = {
                    'lat_min': geo_data['metro_lat'].min(),
                    'lat_max': geo_data['metro_lat'].max(),
                    'lon_min': geo_data['metro_lon'].min(),
                    'lon_max': geo_data['metro_lon'].max()
                }
                
                # –ì—Ä–∞–Ω–∏—Ü—ã –≤–∞–ª–∏–¥–Ω—ã?
                if any(np.isnan(val) for val in self.geo_bounds.values()):
                    print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù–µ–≤–∞–ª–∏–¥–Ω—ã–µ –≥—Ä–∞–Ω–∏—Ü—ã –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
                    df['geo_sin_angle'] = 0
                    df['geo_cos_angle'] = 1
                    return df
                    
                # –î–∏–∞–ø–∞–∑–æ–Ω –Ω–µ–Ω—É–ª–µ–≤–æ–π?
                if self.geo_bounds['lat_max'] == self.geo_bounds['lat_min'] or \
                self.geo_bounds['lon_max'] == self.geo_bounds['lon_min']:
                    print("–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ: –ù—É–ª–µ–≤–æ–π –¥–∏–∞–ø–∞–∑–æ–Ω –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç, —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –∑–Ω–∞—á–µ–Ω–∏—è –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
                    df['geo_sin_angle'] = 0
                    df['geo_cos_angle'] = 1
                    return df
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã—á–∏—Å–ª–µ–Ω–∏–∏ –≥—Ä–∞–Ω–∏—Ü –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç: {e}")
                df['geo_sin_angle'] = 0
                df['geo_cos_angle'] = 1
                return df
        
        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç (–±–∞–∑–∞)
        df['metro_lat_norm'] = (df['metro_lat'] - self.geo_bounds['lat_min']) / (self.geo_bounds['lat_max'] - self.geo_bounds['lat_min'])
        df['metro_lon_norm'] = (df['metro_lon'] - self.geo_bounds['lon_min']) / (self.geo_bounds['lon_max'] - self.geo_bounds['lon_min'])
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –≤ –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö (–∫—Ä–∏–≤–æ)
        df['metro_lat_norm'] = df['metro_lat_norm'].fillna(0)
        df['metro_lon_norm'] = df['metro_lon_norm'].fillna(0)
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –≤ –ø–æ–ª—è—Ä–Ω—ã–µ –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç—ã (–ø–æ –≥–∞–π–¥—É, –Ω–µ —Ç—Ä–æ–≥–∞—Ç—å)
        df['geo_angle'] = np.arctan2(df['metro_lat_norm'], df['metro_lon_norm'])
        df['geo_sin_angle'] = np.sin(df['geo_angle'])
        df['geo_cos_angle'] = np.cos(df['geo_angle'])
        
        # –ó–∞–ø–æ–ª–Ω—è–µ–º NaN –≤ –ø–æ–ª—è—Ä–Ω—ã—Ö –∫–æ–æ—Ä–¥–∏–Ω–∞—Ç–∞—Ö
        df['geo_sin_angle'] = df['geo_sin_angle'].fillna(0)
        df['geo_cos_angle'] = df['geo_cos_angle'].fillna(1)
        
        # –£–¥–∞–ª—è–µ–º –ø—Ä–æ–º–µ–∂—É—Ç–æ—á–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
        df = df.drop(['metro_lat_norm', 'metro_lon_norm', 'geo_angle'], axis=1, errors='ignore')
        
        return df
    
    def prepare_features(self, df, is_training=True):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏—Å–∫–ª—é—á–∞—è 'distance_to_center_km' –∏ 'region'"""
        import numpy as np
        from sklearn.preprocessing import LabelEncoder
        import pandas as pd

        df_features = df.copy()
        
        # –û—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥: –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–Ω –∏ —Å—Ç–æ–ª–±—Ü–æ–≤ (—Ç–æ–ª—å–∫–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è)
        print(f"\nüîç –í prepare_features (is_training={is_training}):")
        print(f"–ò—Å—Ö–æ–¥–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã df: {df_features.columns.tolist()}")
        
        if 'price' in df_features.columns:
            print(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–Ω –≤ df:")
            print(df_features['price'].describe())
            print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω: {df_features['price'].nunique()}")
            print(f"–¢–æ–ø-5 —á–∞—Å—Ç—ã—Ö —Ü–µ–Ω:\n{df_features['price'].value_counts().head()}")
        else:
            print("‚ö†Ô∏è –°—Ç–æ–ª–±–µ—Ü 'price' –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç (—Ä–µ–∂–∏–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è)")
        
        # –ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∏—Å–∫–ª—é—á–∞–µ–º 'region')
        categorical_columns = ['apartment type', 'renovation']
        for col in categorical_columns:
            if col in df_features.columns:
                df_features[col] = df_features[col].astype(str).fillna('unknown')
                if is_training:
                    self.label_encoders[col] = LabelEncoder()
                    df_features[col] = self.label_encoders[col].fit_transform(df_features[col])
                else:
                    if col in self.label_encoders:
                        df_features[col] = df_features[col].map(
                            lambda s: s if s in self.label_encoders[col].classes_ else 'unknown'
                        )
                        if 'unknown' not in self.label_encoders[col].classes_:
                            new_classes = list(self.label_encoders[col].classes_) + ['unknown']
                            self.label_encoders[col].classes_ = np.array(new_classes)
                        df_features[col] = self.label_encoders[col].transform(df_features[col])
            else:
                print(f"‚ö†Ô∏è –°—Ç–æ–ª–±–µ—Ü {col} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ df_features, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
        
        # –ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ (–∏—Å–∫–ª—é—á–∞–µ–º 'distance_to_center_km')
        numeric_columns = ['minutes to metro', 'number of rooms', 'area', 'living area', 
                        'kitchen area', 'floor', 'number of floors', 'metro_lat', 'metro_lon']
        
        # –ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—Å–∫–æ–≤ –º–µ–¥–∏–∞–Ω–æ–π –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        for col in numeric_columns:
            if col in df_features.columns:
                median_value = df_features[col].median()
                df_features[col] = df_features[col].fillna(median_value)
            else:
                print(f"‚ö†Ô∏è –°—Ç–æ–ª–±–µ—Ü {col} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –≤ df_features, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –∏—Å–∫–ª—é—á–∞—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ —Å—Ç–æ–ª–±—Ü—ã
        self.feature_names = [col for col in (numeric_columns + categorical_columns) if col in df_features.columns]
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ price –µ—Å—Ç—å)
        print(f"\nüîç –ü–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –≤ prepare_features:")
        if 'price' in df_features.columns:
            print(f"–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–Ω:")
            print(df_features['price'].describe())
            print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω: {df_features['price'].nunique()}")
            print(f"–¢–æ–ø-5 —á–∞—Å—Ç—ã—Ö —Ü–µ–Ω:\n{df_features['price'].value_counts().head()}")
        
        print(f"–í—ã–±—Ä–∞–Ω–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã (feature_names): {self.feature_names}")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–æ–ª–±—Ü–æ–≤ –≤ X: {len(self.feature_names)}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –≤—Å–µ —Å—Ç–æ–ª–±—Ü—ã –∏–∑ feature_names —Å—É—â–µ—Å—Ç–≤—É—é—Ç
        missing_cols = [col for col in self.feature_names if col not in df_features.columns]
        if missing_cols:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞: —Å—Ç–æ–ª–±—Ü—ã {missing_cols} –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤ df_features!")
            raise KeyError(f"–°—Ç–æ–ª–±—Ü—ã {missing_cols} –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ df_features")
        
        X = df_features[self.feature_names].to_numpy()
        print(f"üîç –†–∞–∑–º–µ—Ä X: {X.shape}")
        return X
    
    def create_price_segments(self, prices, n_segments):
        """–ö–≤–∞–Ω—Ç–∏–ª—åTM"""
        if n_segments == 4:
            # –°–æ–∑–¥–∞–µ–º 4 –∫–≤–∞–Ω—Ç–∏–ª—è -> 5 —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (0-4)
            quantiles = np.quantile(prices, [0.225, 0.45, 0.675, 0.9])
            self.price_quantiles_4 = quantiles
        else:  # n_segments == 3
            # –°–æ–∑–¥–∞–µ–º 3 –∫–≤–∞–Ω—Ç–∏–ª—è -> 4 —Å–µ–≥–º–µ–Ω—Ç–∞ (0-3)
            quantiles = np.quantile(prices, [0.3, 0.6, 0.9])
            self.price_quantiles_3 = quantiles
        
        segments = np.zeros(len(prices), dtype=int)
        for i, q in enumerate(quantiles):
            segments[prices > q] = i + 1
        
        print(f"–°–æ–∑–¥–∞–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(np.unique(segments))}, —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {np.unique(segments)}")
        return segments
    
    def train_neural_network(self, X_train, X_test, y_train, y_test, segment, suffix):
        """–û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –±–µ–∑ –ø–æ–≤—Ç–æ—Ä–Ω–æ–≥–æ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""
        import torch
        import torch.nn as nn
        import torch.optim as optim
        from torch.utils.data import TensorDataset, DataLoader
        from sklearn.preprocessing import StandardScaler
        from sklearn.model_selection import train_test_split
        import numpy as np
        import pandas as pd
        
        print(f"\nüîç –û–±—É—á–µ–Ω–∏–µ –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞ {segment} ({suffix})...")
        print(f"  –§–æ—Ä–º–∞ X_train: {X_train.shape}, y_train: {y_train.shape}")
        print(f"  –§–æ—Ä–º–∞ X_test: {X_test.shape}, y_test: {y_test.shape}")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º y_train –∏ y_test –≤ numpy –º–∞—Å—Å–∏–≤—ã, –µ—Å–ª–∏ –æ–Ω–∏ Series
        if isinstance(y_train, pd.Series):
            y_train = y_train.to_numpy()
        if isinstance(y_test, pd.Series):
            y_test = y_test.to_numpy()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å y
        if y_train.ndim > 1:
            y_train = y_train.ravel()
        if y_test.ndim > 1:
            y_test = y_test.ravel()
        
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –æ–±—É—á–∞—é—â—É—é –∏ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—É—é –≤—ã–±–æ—Ä–∫–∏
        # X_train –£–ñ–ï –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω —á–µ—Ä–µ–∑ main_{suffix}, –Ω–µ —Ç—Ä–æ–≥–∞–µ–º –µ–≥–æ!
        X_train_nn, X_val, y_train_nn, y_val = train_test_split(
            X_train, y_train, test_size=0.2, random_state=42
        )
        print(f"  –§–æ—Ä–º–∞ X_train_nn: {X_train_nn.shape}, X_val: {X_val.shape}")
        print(f"  –§–æ—Ä–º–∞ y_train_nn: {y_train_nn.shape}, y_val: {y_val.shape}")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º y –≤ numpy –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if isinstance(y_train_nn, pd.Series):
            y_train_nn = y_train_nn.to_numpy()
        if isinstance(y_val, pd.Series):
            y_val = y_val.to_numpy()
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –¢–û–õ–¨–ö–û –≤—ã—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—Ü–µ–Ω)
        output_scaler = StandardScaler()
        y_train_scaled = output_scaler.fit_transform(y_train_nn.reshape(-1, 1)).ravel()
        y_val_scaled = output_scaler.transform(y_val.reshape(-1, 1)).ravel()
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ü–µ–Ω
        print(f"  –¶–µ–Ω—ã –≤ y_train_nn:\n{pd.Series(y_train_nn).describe()}")
        print(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω –≤ y_train_nn: {np.unique(y_train_nn).size}")
        
        # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
        batch_size = 32
        num_epochs = 100
        patience = 10
        
        # –ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –≤ —Ç–µ–Ω–∑–æ—Ä—ã (X —É–∂–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω!)
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
        
        X_train_tensor = torch.tensor(X_train_nn, dtype=torch.float32).to(device)
        y_train_tensor = torch.tensor(y_train_scaled, dtype=torch.float32).to(device)
        X_val_tensor = torch.tensor(X_val, dtype=torch.float32).to(device)
        y_val_tensor = torch.tensor(y_val_scaled, dtype=torch.float32).to(device)
        X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)
        
        # –°–æ–∑–¥–∞–µ–º DataLoader
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(
            train_dataset, 
            batch_size=batch_size, 
            shuffle=True,
            drop_last=True
        )
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏
        model = ImprovedPricePredictor(
            input_size=X_train_nn.shape[1],  # –ò–°–ü–†–ê–í–õ–ï–ù–û: –±—ã–ª–æ X_train_scaled
            dropout_rate=0.2
        ).to(device)
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        
        best_val_loss = float('inf')
        patience_counter = 0
        
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º batch_size = {batch_size} (–¥–∞–Ω–Ω—ã—Ö: {len(X_train_nn)})")
        print(f"–ë–∞—Ç—á–µ–π –∑–∞ —ç–ø–æ—Ö—É: {len(train_loader)}")
        print("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ...")
        
        # –û–±—É—á–µ–Ω–∏–µ
        for epoch in range(num_epochs):
            model.train()
            epoch_loss = 0.0
            
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs.squeeze(), batch_y)
                loss.backward()
                optimizer.step()
                epoch_loss += loss.item()
            
            avg_train_loss = epoch_loss / len(train_loader)
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            model.eval()
            with torch.no_grad():
                val_outputs = model(X_val_tensor)
                val_loss = criterion(val_outputs.squeeze(), y_val_tensor)
            
            if epoch % 20 == 0 or epoch == num_epochs - 1:
                print(f"Epoch {epoch:3d}: Train Loss: {avg_train_loss:.6f}, Val Loss: {val_loss.item():.6f}")
            
            # –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                torch.save(model.state_dict(), os.path.join(self.models_dir, f'nn_{segment}_{suffix}.pth'))
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    print(f"–†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch}")
                    break
        
        print("–û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
        
        # –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        model.eval()
        with torch.no_grad():
            y_pred_scaled = model(X_test_tensor).cpu().numpy().ravel()
            y_pred = output_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()
        
        r2_nn = r2_score(y_test, y_pred)
        rmse_nn = np.sqrt(mean_squared_error(y_test, y_pred))
        mae_nn = mean_absolute_error(y_test, y_pred)
        
        model_key = f'nn_{segment}_{suffix}'
        print(f"–°–µ–≥–º–µ–Ω—Ç {segment} NN: R¬≤ = {r2_nn:.4f}, RMSE = {rmse_nn:.0f}, MAE = {mae_nn:.0f}")
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç—Ä–∏–∫–∏
        self.model_metrics[model_key] = {
            'R¬≤': r2_nn, 
            'RMSE': rmse_nn, 
            'MAE': mae_nn,
            'Samples': len(y_test)
        }
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
        self.models[model_key] = model
        print(f"üíæ –ú–æ–¥–µ–ª—å {model_key} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ self.models")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–æ–ª—å–∫–æ output_scaler
        scalers_dict = {
            'output': output_scaler
        }
        self.scalers[f'y_scaler_{model_key}'] = scalers_dict
        
        print(f"üíæ –°–∫–µ–π–ª–µ—Ä —Å–æ—Ö—Ä–∞–Ω–µ–Ω: y_scaler_{model_key}")
        
        if model_key in self.models:
            print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_key} —É—Å–ø–µ—à–Ω–æ –¥–æ–±–∞–≤–ª–µ–Ω–∞ –≤ self.models")
        else:
            print(f"‚ùå –û–®–ò–ë–ö–ê: –ú–æ–¥–µ–ª—å {model_key} –ù–ï —Å–æ—Ö—Ä–∞–Ω–∏–ª–∞—Å—å –≤ self.models!")
        
        return model, {'R¬≤': r2_nn, 'RMSE': rmse_nn, 'MAE': mae_nn}
        
    def predict_neural_network(self, model, X_scaled, y_scaler=None):
        """
        –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –ø–æ–º–æ—â—å—é —É–ª—É—á—à–µ–Ω–Ω–æ–π –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏
        X_scaled - —É–∂–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ —á–µ—Ä–µ–∑ main_{data_type} scaler
        """
        if model is None or y_scaler is None:
            print("Error: Model or y_scaler is None")
            return None
        
        model.eval()
        
        # X —É–∂–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω –≤ predict_price, –Ω–µ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º –ø–æ–≤—Ç–æ—Ä–Ω–æ!
        X_scaled = np.nan_to_num(X_scaled, nan=0, posinf=1e6, neginf=-1e6)
        
        with torch.no_grad():
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            X_tensor = torch.FloatTensor(X_scaled).to(device)
            predictions_scaled = model(X_tensor).squeeze().numpy()
        
        # Ensure predictions_scaled is 1D
        if predictions_scaled.ndim > 1:
            predictions_scaled = predictions_scaled.ravel()
        
        # Debug: Log scaled predictions
        print(f"  [DEBUG] pred_scaled range: [{np.min(predictions_scaled):.4f}, {np.max(predictions_scaled):.4f}]")
        
        # –û–±—Ä–∞—Ç–Ω–æ–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–ª—å–∫–æ –¥–ª—è Y
        predictions = y_scaler['output'].inverse_transform(
            predictions_scaled.reshape(-1, 1)
        ).flatten()
        
        # Debug: Log unscaled predictions
        print(f"  [DEBUG] pred range: [{np.min(predictions):,.0f}, {np.max(predictions):,.0f}]")
        
        # –£–±–∏—Ä–∞–µ–º –æ—Ç—Ä–∏—Ü–∞—Ç–µ–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
        predictions = np.maximum(predictions, 0)
        
        return predictions
        
    def plot_training_history(self, train_losses, val_losses):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏—Å—Ç–æ—Ä–∏–∏ –æ–±—É—á–µ–Ω–∏—è"""
        import matplotlib
        matplotlib.use('Agg')  # Use non-interactive Agg backend (—á–µ–∫, –Ω–æ  –±–µ–∑  —ç—Ç–æ–≥–æ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç)
        import matplotlib.pyplot as plt
            
        plt.figure(figsize=(12, 4))
            
        plt.subplot(1, 2, 1)
        plt.plot(train_losses, label='Train Loss')
        plt.plot(val_losses, label='Validation Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training History')
        plt.legend()
        plt.grid(True)
      
        plt.subplot(1, 2, 2)
        plt.plot(train_losses[-50:], label='Train Loss (last 50)')
        plt.plot(val_losses[-50:], label='Validation Loss (last 50)')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.title('Training History (Last 50 epochs)')
        plt.legend()
        plt.grid(True)
            
        plt.tight_layout()
            
        # Save the plot to a file instead of displaying it
        output_path = os.path.join(self.models_dir, 'training_history.png')
        plt.savefig(output_path)
        plt.close()  # Close the figure to free memory
        print(f"Training history plot saved to {output_path}")
        
    def evaluate_model(self, y_true, y_pred, model_name):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –º–æ–¥–µ–ª–∏"""
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)
            
        # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω—é—é –∞–±—Å–æ–ª—é—Ç–Ω—É—é –æ—à–∏–±–∫—É –¥–ª—è –¥–∏–∞–ø–∞–∑–æ–Ω–∞, mean_abs_error –Ω–µ mean_absolute_error, —ç—Ç–∞ –¥–ª—è intervallo (—á–µ–∫, –¥—É–±–ª–∏–∫–∞—Ç)
        abs_errors = np.abs(y_true - y_pred)
        mean_abs_error = np.mean(abs_errors)
            
        self.model_metrics[model_name] = {
            'R¬≤': round(r2, 4),
            'RMSE': round(rmse, 0),
            'MAE': round(mae, 0),
            'Samples': len(y_true)
        }
            
        self.model_errors[model_name] = mean_abs_error
            
        return r2, rmse, mae
        
    def save_models(self):
        pytorch_models = {}
        sklearn_models = {}
        
        for key, model in self.models.items():
            if isinstance(model, nn.Module):
                pytorch_models[key] = {
                    'state_dict': model.state_dict(),
                    'input_size': model.input_layer.in_features,
                    'dropout_rate': 0.2
                }
                pth_path = os.path.join(self.models_dir, f'{key}.pth')
                torch.save(model.state_dict(), pth_path)
                print(f"üíæ PyTorch –º–æ–¥–µ–ª—å {key} —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {pth_path}")
                if not os.path.exists(pth_path):
                    print(f"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª {pth_path} –Ω–µ —Å–æ–∑–¥–∞–Ω!")
            else:
                sklearn_models[key] = model

        metadata = {
            'models': sklearn_models,
            'pytorch_configs': pytorch_models,
            'scalers': self.scalers,
            'label_encoders': self.label_encoders,
            'feature_names': self.feature_names,
            'price_quantiles_4': self.price_quantiles_4,
            'price_quantiles_3': self.price_quantiles_3,
            'geo_bounds': self.geo_bounds,
            'model_metrics': self.model_metrics,
            'model_errors': self.model_errors,
            'training_data': self.training_data,
            'training_data_numeric': self.training_data_numeric,
            'renovation_mapping': self.renovation_mapping
        }
        
        filepath = os.path.join(self.models_dir, 'metadata.pkl')
        with open(filepath, 'wb') as f:
            pickle.dump(metadata, f)
        print(f"‚úÖ –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filepath}")
        if not os.path.exists(filepath):
            print(f"‚ùå –û—à–∏–±–∫–∞: —Ñ–∞–π–ª {filepath} –Ω–µ —Å–æ–∑–¥–∞–Ω!")

    def load_models(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–µ–π –∏ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö"""
        import torch
        import torch.nn as nn
        import pandas as pd
        
        filepath = os.path.join(self.models_dir, 'metadata.pkl')
        
        try:
            with open(filepath, 'rb') as f:
                metadata = pickle.load(f)
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º sklearn –º–æ–¥–µ–ª–∏
            self.models = metadata.get('models', {})  # —Å–Ω–∞—á–∞–ª–∞ sklearn
            sklearn_count = len(self.models)
            
            self.scalers = metadata.get('scalers', {})
            self.label_encoders = metadata.get('label_encoders', {})
            self.feature_names = metadata.get('feature_names', [])
            self.price_quantiles_4 = metadata.get('price_quantiles_4')
            self.price_quantiles_3 = metadata.get('price_quantiles_3')
            self.geo_bounds = metadata.get('geo_bounds')
            self.model_metrics = metadata.get('model_metrics', {})
            self.model_errors = metadata.get('model_errors', {})
            self.training_data = metadata.get('training_data', None)
            self.training_data_numeric = metadata.get('training_data_numeric', None)
            self.renovation_mapping = metadata.get('renovation_mapping', None)
            self.feature_quantiles = metadata.get('feature_quantiles', None)  # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–≥—Ä—É–∑–∫—É feature_quantiles
            
            # –ó–∞–≥—Ä—É–∂–∞–µ–º PyTorch –º–æ–¥–µ–ª–∏ –∏ –¥–æ–±–∞–≤–ª—è–µ–º –∫ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–º
            pytorch_configs = metadata.get('pytorch_configs', {})
            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            pytorch_count = 0
            
            for key, config in pytorch_configs.items():
                try:
                    model = ImprovedPricePredictor(
                        input_size=config['input_size'],
                        dropout_rate=config.get('dropout_rate', 0.2)
                    ).to(device)
                    model.load_state_dict(config['state_dict'])
                    model.eval()
                    self.models[key] = model
                    pytorch_count += 1
                    print(f"‚úÖ PyTorch –º–æ–¥–µ–ª—å {key} –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ PyTorch –º–æ–¥–µ–ª–∏ {key}: {e}")
            
            print(f"\n‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑ {filepath}:")
            print(f"   Sklearn –º–æ–¥–µ–ª–µ–π: {sklearn_count}")
            print(f"   PyTorch –º–æ–¥–µ–ª–µ–π: {pytorch_count}")
            print(f"   –í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π: {len(self.models)}")
            print(f"   –ú–µ—Ç—Ä–∏–∫: {len(self.model_metrics)}")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏
            if len(self.models) != sklearn_count + pytorch_count:
                print(f"‚ö†Ô∏è –í–ù–ò–ú–ê–ù–ò–ï: –û–∂–∏–¥–∞–ª–æ—Å—å {sklearn_count + pytorch_count} –º–æ–¥–µ–ª–µ–π, –∑–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.models)}")
            
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è feature_quantiles –∏ training_data, –µ—Å–ª–∏ –æ–Ω–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç
            if self.training_data is None or self.feature_quantiles is None:
                print("\n‚ö†Ô∏è training_data –∏–ª–∏ feature_quantiles –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç, –ø—ã—Ç–∞–µ–º—Å—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å...")
                try:
                    df = pd.read_csv(r'C:\Users\nikit\Desktop\–ü—Ä–æ–µ–∫—Ç (25 —Å–µ–Ω—Ç—è–±—Ä—è)\newdata.csv', encoding='utf-8')
                    df.columns = df.columns.str.strip().str.lower()
                    required_columns = [
                        'price', 'apartment type', 'minutes to metro', 'number of rooms', 
                        'area', 'living area', 'kitchen area', 'floor', 'number of floors',
                        'renovation', 'metro_lat', 'metro_lon'
                    ]
                    df = df[required_columns]
                    df = df.dropna(subset=['price'])
                    self.init_similarity_search(df)
                    print("‚úÖ training_data –∏ feature_quantiles —É—Å–ø–µ—à–Ω–æ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
                except Exception as e:
                    print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ training_data: {e}")
                    self.feature_quantiles = {}  # –ü—É—Å—Ç–æ–π —Å–ª–æ–≤–∞—Ä—å –∫–∞–∫ fallback
            
            return True
            
        except FileNotFoundError:
            print(f"‚ùå –§–∞–π–ª {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω. –¢—Ä–µ–±—É–µ—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π.")
            return False
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–µ–π: {e}")
            import traceback
            traceback.print_exc()
            return False
                
        except FileNotFoundError:
            print(f"‚ùå –§–∞–π–ª {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω. –¢—Ä–µ–±—É–µ—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π.")
            return False
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–µ–π: {e}")
            import traceback
            traceback.print_exc()
            return False
            
        except FileNotFoundError:
            print(f"‚ùå –§–∞–π–ª {filepath} –Ω–µ –Ω–∞–π–¥–µ–Ω. –¢—Ä–µ–±—É–µ—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π.")
            return False
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–µ–π: {e}")
            return False

    def train_models(self, df, use_cleaned_data=True):
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ü–µ–Ω"""
        print("–ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        print(f"\nüîç –ò—Å—Ö–æ–¥–Ω—ã–µ —Ü–µ–Ω—ã –≤ df['price']:")
        print(df['price'].describe())
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω: {df['price'].nunique()}")
        print(f"–¢–æ–ø-5 —á–∞—Å—Ç—ã—Ö —Ü–µ–Ω:\n{df['price'].value_counts().head()}")
        print(f"–ò—Å—Ö–æ–¥–Ω—ã–µ —Å—Ç–æ–ª–±—Ü—ã df: {df.columns.tolist()}")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è similarity search (–¥–ª—è renovation_mapping)
        print("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–∏—Å–∫–∞ –ø–æ—Ö–æ–∂–∏—Ö –æ–±—ä–µ–∫—Ç–æ–≤...")
        self.init_similarity_search(df)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        X = self.prepare_features(df, is_training=True)
        y = df['price']
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ prepare_features
        print(f"\nüîç –¶–µ–Ω—ã –≤ y (–ø–æ—Å–ª–µ prepare_features):")
        print(y.describe())
        print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω: {y.nunique()}")
        print(f"–¢–æ–ø-5 —á–∞—Å—Ç—ã—Ö —Ü–µ–Ω:\n{y.value_counts().head()}")
        print(f"–†–∞–∑–º–µ—Ä X: {X.shape}, –û–∂–∏–¥–∞–µ–º—ã–µ —Å—Ç–æ–ª–±—Ü—ã: {self.feature_names}")
        
        # –û—á–∏—Å—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        if use_cleaned_data:
            print("\n=== –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ===")
            data_type = 'clean'
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º self.feature_names –≤–º–µ—Å—Ç–æ df.columns.drop('price')
            df_temp = pd.DataFrame(X, columns=self.feature_names)
            df_temp['price'] = y.reset_index(drop=True)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ df_temp
            print(f"\nüîç –¶–µ–Ω—ã –≤ df_temp (–ø–µ—Ä–µ–¥ clean_data):")
            print(df_temp['price'].describe())
            print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω: {df_temp['price'].nunique()}")
            print(f"–¢–æ–ø-5 —á–∞—Å—Ç—ã—Ö —Ü–µ–Ω:\n{df_temp['price'].value_counts().head()}")
            print(f"–°—Ç–æ–ª–±—Ü—ã df_temp: {df_temp.columns.tolist()}")
            
            df_clean = self.clean_data(df_temp)
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ clean_data
            print(f"\nüîç –¶–µ–Ω—ã –≤ df_clean (–ø–æ—Å–ª–µ clean_data):")
            print(df_clean['price'].describe())
            print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω: {df_clean['price'].nunique()}")
            print(f"–¢–æ–ø-5 —á–∞—Å—Ç—ã—Ö —Ü–µ–Ω:\n{df_clean['price'].value_counts().head()}")
            
            X_clean = df_clean.drop('price', axis=1).to_numpy()
            y_clean = df_clean['price'].to_numpy()
            
            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö
            X_train, X_test, y_train, y_test = train_test_split(X_clean, y_clean, test_size=0.2, random_state=42)
            print(f"\nüîç –¶–µ–Ω—ã –≤ y_train (–ø–æ—Å–ª–µ split):")
            print(pd.Series(y_train).describe())
            print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω –≤ y_train: {np.unique(y_train).size}")
            print(f"\nüîç –¶–µ–Ω—ã –≤ y_test (–ø–æ—Å–ª–µ split):")
            print(pd.Series(y_test).describe())
            print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω –≤ y_test: {np.unique(y_test).size}")
        else:
            print("\n=== –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ===")
            data_type = 'raw'
            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
            print(f"\nüîç –¶–µ–Ω—ã –≤ y_train (raw, –ø–æ—Å–ª–µ split):")
            print(pd.Series(y_train).describe())
            print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω –≤ y_train: {np.unique(y_train).size}")
            print(f"\nüîç –¶–µ–Ω—ã –≤ y_test (raw, –ø–æ—Å–ª–µ split):")
            print(pd.Series(y_test).describe())
            print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω –≤ y_test: {np.unique(y_test).size}")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        self.training_data = {
            'X_train': X_train.copy(),
            'y_train': y_train.copy(),
            'X_test': X_test.copy(),
            'y_test': y_test.copy()
        }
        
        # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Ç–æ–ª—å–∫–æ X, –Ω–µ y)
        scaler = StandardScaler()
        X_train_scaled = scaler.fit_transform(X_train)
        X_test_scaled = scaler.transform(X_test)
        self.scalers[f'main_{data_type}'] = scaler
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        self.training_data_numeric = {
            'X_train': X_train_scaled,
            'y_train': y_train,
            'X_test': X_test_scaled,
            'y_test': y_test
        }
        
        # –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∫–≤–∞–Ω—Ç–∏–ª–µ–π
        self.price_quantiles_4 = np.percentile(y_train, [22.5, 45, 67.5, 90])
        self.price_quantiles_3 = np.percentile(y_train, [30, 60, 90])
        self.price_quantile_90 = np.percentile(y_train, 80)  # –ò—Å–ø–æ–ª—å–∑—É–µ–º 80% –¥–ª—è –±–æ–ª—å—à–µ–π –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
        print(f"–ö–≤–∞—Ä—Ç–∏–ª–∏ –¥–ª—è 4 —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {self.price_quantiles_4}")
        print(f"–ö–≤–∞—Ä—Ç–∏–ª–∏ –¥–ª—è 3 —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {self.price_quantiles_3}")
        print(f"–ö–≤–∞–Ω—Ç–∏–ª—å 80%: {self.price_quantile_90}")
        
        # –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –∏ —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤
        for n_segments, quantiles in [
            (4, self.price_quantiles_4),
            (3, self.price_quantiles_3),
            (2, [self.price_quantile_90])
        ]:
            segments = self.create_price_segments_with_quantiles(y_train, quantiles, n_segments)
            segments_test = self.create_price_segments_with_quantiles(y_test, quantiles, n_segments)
            suffix = f"{n_segments}seg_{data_type}"
            self.train_classification_system(X_train_scaled, X_test_scaled, y_train, y_test, segments, segments_test, suffix)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
        self.save_models()


    def train_classification_system(self, X_train, X_test, y_train, y_test, segments, segments_test, suffix):
        """–û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–æ–≤ –∏ —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏"""
        print(f"\n=== –û–±—É—á–µ–Ω–∏–µ —Å–∏—Å—Ç–µ–º—ã ({suffix}) ===")
        n_segments = len(np.unique(segments))
        print(f"–°–æ–∑–¥–∞–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {n_segments}, —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {np.unique(segments)}")
        
        # –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞
        clf = RandomForestClassifier(n_estimators=100, random_state=42)
        clf.fit(X_train, segments)
        self.models[f'classifier_{suffix}'] = clf
        y_pred = clf.predict(X_test)
        accuracy = accuracy_score(segments_test, y_pred)
        print(f"–¢–æ—á–Ω–æ—Å—Ç—å –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏ ({suffix}): {accuracy:.3f}")
        
        # –û–±—É—á–µ–Ω–∏–µ —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞
        for segment in range(n_segments):
            print(f"–û–±—É—á–µ–Ω–∏–µ —Ä–µ–≥—Ä–µ—Å—Å–æ—Ä–æ–≤ –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞ {segment} ({suffix})...")
            segment_mask_train = segments == segment
            segment_mask_test = segments_test == segment
            X_segment_train = X_train[segment_mask_train]
            y_segment_train = y_train.iloc[segment_mask_train] if hasattr(y_train, 'iloc') else y_train[segment_mask_train]
            X_segment_test = X_test[segment_mask_test]
            y_segment_test = y_test.iloc[segment_mask_test] if hasattr(y_test, 'iloc') else y_test[segment_mask_test]
            
            # –û—Ç–ª–∞–¥–æ—á–Ω—ã–π –≤—ã–≤–æ–¥
            print(f"\nüîç –°–µ–≥–º–µ–Ω—Ç {segment} ({suffix}):")
            print(f"  Train: {len(X_segment_train)} –æ–±—ä–µ–∫—Ç–æ–≤, Test: {len(X_segment_test)} –æ–±—ä–µ–∫—Ç–æ–≤")
            print(f"  –¶–µ–Ω—ã –≤ y_segment_train:\n{pd.Series(y_segment_train).describe()}")
            print(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω –≤ y_segment_train: {np.unique(y_segment_train).size}")
            print(f"  –¶–µ–Ω—ã –≤ y_segment_test:\n{pd.Series(y_segment_test).describe()}")
            print(f"  –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω –≤ y_segment_test: {np.unique(y_segment_test).size}")
            
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–µ–≥–º–µ–Ω—Ç, –µ—Å–ª–∏ –Ω–µ—Ç –≤–∞—Ä–∏–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏ —Ü–µ–Ω
            if np.unique(y_segment_train).size < 2 or np.unique(y_segment_test).size < 2:
                print(f"‚ö†Ô∏è –°–µ–≥–º–µ–Ω—Ç {segment} —Å–æ–¥–µ—Ä–∂–∏—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
                continue
            
            # RandomForest
            rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
            rf_model.fit(X_segment_train, y_segment_train)
            self.models[f'rf_{segment}_{suffix}'] = rf_model
            y_pred_rf = rf_model.predict(X_segment_test)
            
            # –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ RF
            print(f"  üîç –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ RF –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞ {segment}:")
            print(f"    y_test range: [{np.min(y_segment_test):,.0f}, {np.max(y_segment_test):,.0f}]")
            print(f"    y_pred range: [{np.min(y_pred_rf):,.0f}, {np.max(y_pred_rf):,.0f}]")
            print(f"    y_test mean: {np.mean(y_segment_test):,.0f}, y_pred mean: {np.mean(y_pred_rf):,.0f}")
            print(f"    –ü–µ—Ä–≤—ã–µ 5 y_test: {y_segment_test[:5].tolist()}")
            print(f"    –ü–µ—Ä–≤—ã–µ 5 y_pred: {y_pred_rf[:5].tolist()}")
            
            r2_rf = r2_score(y_segment_test, y_pred_rf)
            rmse_rf = np.sqrt(mean_squared_error(y_segment_test, y_pred_rf))
            mae_rf = mean_absolute_error(y_segment_test, y_pred_rf)
            print(f"–°–µ–≥–º–µ–Ω—Ç {segment} RF: R¬≤ = {r2_rf:.4f}, RMSE = {rmse_rf:.0f}, MAE = {mae_rf:.0f}")
            self.model_metrics[f'rf_{segment}_{suffix}'] = {'R¬≤': r2_rf, 'RMSE': rmse_rf, 'MAE': mae_rf}
            
            # XGBoost
            xgb_model = XGBRegressor(n_estimators=100, random_state=42)
            xgb_model.fit(X_segment_train, y_segment_train)
            self.models[f'xgb_{segment}_{suffix}'] = xgb_model
            y_pred_xgb = xgb_model.predict(X_segment_test)
            
            r2_xgb = r2_score(y_segment_test, y_pred_xgb)
            rmse_xgb = np.sqrt(mean_squared_error(y_segment_test, y_pred_xgb))
            mae_xgb = mean_absolute_error(y_segment_test, y_pred_xgb)
            print(f"–°–µ–≥–º–µ–Ω—Ç {segment} XGB: R¬≤ = {r2_xgb:.4f}, RMSE = {rmse_xgb:.0f}, MAE = {mae_xgb:.0f}")
            self.model_metrics[f'xgb_{segment}_{suffix}'] = {'R¬≤': r2_xgb, 'RMSE': rmse_xgb, 'MAE': mae_xgb}
            
            # –ù–µ–π—Ä–æ–Ω–Ω–∞—è —Å–µ—Ç—å
            self.train_neural_network(X_segment_train, X_segment_test, y_segment_train, y_segment_test, segment, suffix)


    def create_price_segments_with_quantiles(self, prices, quantiles, n_segments):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Å–µ–≥–º–µ–Ω—Ç–æ–≤ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –≥–æ—Ç–æ–≤—ã—Ö –∫–≤–∞–Ω—Ç–∏–ª–µ–π"""
        segments = np.zeros(len(prices), dtype=int)
        for i, q in enumerate(quantiles):
            segments[prices > q] = i + 1
        print(f"–°–æ–∑–¥–∞–Ω–æ —Å–µ–≥–º–µ–Ω—Ç–æ–≤: {len(np.unique(segments))}, —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è: {np.unique(segments)}")
        return segments
    
    def print_metrics_table(self):
        """–í—ã–≤–æ–¥ —Ç–∞–±–ª–∏—Ü—ã –º–µ—Ç—Ä–∏–∫ –≤—Å–µ—Ö –º–æ–¥–µ–ª–µ–π"""
        print("\n" + "="*80)
        print("–ú–ï–¢–†–ò–ö–ò –í–°–ï–• –ú–û–î–ï–õ–ï–ô –†–ï–ì–†–ï–°–°–ò–ò")
        print("="*80)
        
        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã (—á–µ–∫, –∫—É—á–∞ –∞–Ω–æ–º–∞–ª–∏–π –≤ —Ü–µ–Ω—Ç—Ä–µ)
        table_data = []
        for model_name, metrics in self.model_metrics.items():
            table_data.append([
                model_name,
                metrics['R¬≤'],
                f"{metrics['RMSE']:.0f}",
                f"{metrics['MAE']:.0f}"
            ])
        
        headers = ['–ú–æ–¥–µ–ª—å', 'R¬≤', 'RMSE', 'MAE', '–í—ã–±–æ—Ä–∫–∞']
        print(tabulate(table_data, headers=headers, tablefmt='grid', stralign='center'))
    
    def get_price_range_for_segment(self, segment, system_type):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ —Ü–µ–Ω –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–∞"""
        if '4seg' in system_type:
            quantiles = self.price_quantiles_4
        else:
            quantiles = self.price_quantiles_3
        
        if quantiles is None:
            return "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω"
        
        if '4seg' in system_type:
            ranges = [
                f"–¥–æ {quantiles[0]:,.0f} —Ä—É–±.",
                f"{quantiles[0]:,.0f} - {quantiles[1]:,.0f} —Ä—É–±.",
                f"{quantiles[1]:,.0f} - {quantiles[2]:,.0f} —Ä—É–±.",
                f"–æ—Ç {quantiles[2]:,.0f} —Ä—É–±."
            ]
        else:  # 3seg
            ranges = [
                f"–¥–æ {quantiles[0]:,.0f} —Ä—É–±.",
                f"{quantiles[0]:,.0f} - {quantiles[1]:,.0f} —Ä—É–±.",
                f"–æ—Ç {quantiles[1]:,.0f} —Ä—É–±."
            ]
        
        return ranges[segment] if segment < len(ranges) else "–ù–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω"
    
    def predict_ensemble(self, X_scaled, model_keys, model_type='standard'):
        predictions = []
        valid_models = 0
        errors = []
        successful_keys = []
        model_names = []
        
        for model_key in model_keys:
            if model_key not in self.models:
                print(f"–ú–æ–¥–µ–ª—å {model_key} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
                continue
            
            model = self.models[model_key]
            metrics = self.model_metrics.get(model_key, {})
            model_r2 = metrics.get('R¬≤', metrics.get('R2', 0.7))
            
            if model_r2 < 0.0:
                print(f"–ú–æ–¥–µ–ª—å {model_key} –∏–º–µ–µ—Ç R¬≤ = {model_r2:.4f} < 0.0, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
                continue
            
            try:
                if isinstance(model, nn.Module):
                    y_scaler_key = f'y_scaler_{model_key}'
                    scalers_dict = self.scalers.get(y_scaler_key, None)
                    
                    if scalers_dict is None:
                        print(f"Y-—Å–∫–µ–π–ª–µ—Ä –¥–ª—è {model_key} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
                        continue
                    
                    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
                    model.eval()
                    
                    with torch.no_grad():
                        X_tensor = torch.tensor(X_scaled, dtype=torch.float32).to(device)
                        pred_scaled = model(X_tensor).cpu().numpy().ravel()
                    
                    output_scaler = scalers_dict['output']
                    pred = output_scaler.inverse_transform(pred_scaled.reshape(-1, 1)).ravel()
                    
                    if len(predictions) >= 2:
                        other_median = np.median(predictions)
                        deviation_pct = abs(pred[0] - other_median) / other_median
                        if deviation_pct > 0.5:
                            print(f"  ‚ö†Ô∏è NN –æ—Ç–∫–ª–æ–Ω—è–µ—Ç—Å—è –Ω–∞ {deviation_pct*100:.0f}% –æ—Ç –º–µ–¥–∏–∞–Ω—ã –¥—Ä—É–≥–∏—Ö –º–æ–¥–µ–ª–µ–π")
                            continue
                    
                    if np.any(np.isnan(pred)):
                        print(f"  –û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–µ–π—Ä–æ–Ω–Ω–æ–π —Å–µ—Ç–∏ –¥–ª—è {model_key}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
                        continue
                    
                    predictions.append(pred[0])
                    errors.append(1 - model_r2)
                    successful_keys.append(model_key)
                    
                else:
                    pred = model.predict(X_scaled)
                    
                    if np.any(np.isnan(pred)):
                        print(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è {model_key}, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
                        continue
                    
                    predictions.append(pred[0])
                    errors.append(1 - model_r2)
                    successful_keys.append(model_key)
                
                valid_models += 1
                
                if 'nn' in model_key:
                    algo_name = 'NN'
                elif 'rf' in model_key:
                    algo_name = 'RF'
                elif 'xgb' in model_key:
                    algo_name = 'XGB'
                else:
                    algo_name = model_key.split('_')[-1].upper()
                
                model_names.append(algo_name)
                print(f"{algo_name} R¬≤: {model_r2:.4f}, –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {predictions[-1]:,.0f} —Ä—É–±.")
            
            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏ –¥–ª—è {model_key}: {e}")
                continue
        
        if valid_models == 0:
            print("–ù–µ—Ç –≤–∞–ª–∏–¥–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∞–Ω—Å–∞–º–±–ª—è!")
            return None, {}
        
        errors = np.array(errors)
        errors = np.clip(errors, 1e-8, None)
        weights = 1 / (errors ** 2)
        weights = weights / np.sum(weights)
        
        weights_dict = dict(zip(model_names, weights.round(4)))
        print(f"\n–í–µ—Å–∞ –º–æ–¥–µ–ª–µ–π: {weights_dict}")
        print(f"–°—É–º–º–∞ –≤–µ—Å–æ–≤: {np.sum(weights):.4f}")
        print(f"–ò–Ω–¥–∏–≤–∏–¥—É–∞–ª—å–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è: {[f'{p:,.0f}' for p in predictions]}")
        
        ensemble_prediction = np.average(predictions, weights=weights)
        print(f"–í–∑–≤–µ—à–µ–Ω–Ω–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª—è: {ensemble_prediction:,.0f} —Ä—É–±.")
        
        return ensemble_prediction, weights_dict
        
    def calculate_prediction_range(self, ensemble_prediction, model_keys):
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ MAE"""
        errors = []
        
        for model_key in model_keys:
            if model_key in self.model_errors:
                errors.append(self.model_errors[model_key])
        
        if not errors:
            # –ï—Å–ª–∏ –Ω–µ—Ç –¥–∞–Ω–Ω—ã—Ö –æ–± –æ—à–∏–±–∫–∞—Ö, –∏—Å–ø–æ–ª—å–∑—É–µ–º 10% –æ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è (—á–µ–∫, –Ω–µ –∑–∞—Å–∫–∞–º—å —Å–µ–±—è)
            error_margin = ensemble_prediction * 0.1
        else:
            error_margin = np.mean(errors)
        
        lower_bound = max(0, ensemble_prediction - error_margin)
        upper_bound = ensemble_prediction + error_margin
        
        return lower_bound, upper_bound
    
    def predict_price(self, property_data, use_cleaned_models=True):
        if not self.models:
            raise ValueError("–ú–æ–¥–µ–ª–∏ –Ω–µ –æ–±—É—á–µ–Ω—ã! –°–Ω–∞—á–∞–ª–∞ –æ–±—É—á–∏—Ç–µ –º–æ–¥–µ–ª–∏ –∏–ª–∏ –∑–∞–≥—Ä—É–∑–∏—Ç–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–Ω—ã–µ.")
        
        data_suffix = 'clean' if use_cleaned_models else 'raw'
        print(f"\n--- –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –º–æ–¥–µ–ª—è–º–∏ –Ω–∞ {'–æ—á–∏—â–µ–Ω–Ω—ã—Ö' if use_cleaned_models else '–∏—Å—Ö–æ–¥–Ω—ã—Ö'} –¥–∞–Ω–Ω—ã—Ö ---")
        
        df_input = pd.DataFrame([property_data])
        X = self.prepare_features(df_input, is_training=False)
        
        expected_features = 11
        if X.shape[1] != expected_features:
            raise ValueError(f"–û–∂–∏–¥–∞–ª–æ—Å—å {expected_features} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, –Ω–æ –ø–æ–ª—É—á–µ–Ω–æ {X.shape[1]}")
        
        scaler_key = f'main_{data_suffix}'
        if scaler_key not in self.scalers:
            scaler_key = list(self.scalers.keys())[0]
        
        X_scaled = self.scalers[scaler_key].transform(X)
        
        classifier_4_key = f'classifier_4seg_{data_suffix}'
        seg4_pred, seg4_proba, seg4_confidence = 0, [1.0], 0.0
        if classifier_4_key in self.models:
            seg4_pred = int(self.models[classifier_4_key].predict(X_scaled)[0])
            seg4_proba = self.models[classifier_4_key].predict_proba(X_scaled)[0]
            seg4_confidence = float(np.max(seg4_proba))
        
        classifier_3_key = f'classifier_3seg_{data_suffix}'
        seg3_pred, seg3_proba, seg3_confidence = 0, [1.0], 0.0
        if classifier_3_key in self.models:
            seg3_pred = int(self.models[classifier_3_key].predict(X_scaled)[0])
            seg3_proba = self.models[classifier_3_key].predict_proba(X_scaled)[0]
            seg3_confidence = float(np.max(seg3_proba))
        
        available_model_keys = []
        if seg4_confidence >= 0.8:
            model_keys = [
                f'rf_{seg4_pred}_4seg_{data_suffix}',
                f'xgb_{seg4_pred}_4seg_{data_suffix}',
                f'nn_{seg4_pred}_4seg_{data_suffix}'
            ]
            chosen_system = f'4seg_{data_suffix}'
            chosen_segment = seg4_pred
            confidence = seg4_confidence
        elif seg3_confidence >= 0.8:
            model_keys = [
                f'rf_{seg3_pred}_3seg_{data_suffix}',
                f'xgb_{seg3_pred}_3seg_{data_suffix}',
                f'nn_{seg3_pred}_3seg_{data_suffix}'
            ]
            chosen_system = f'3seg_{data_suffix}'
            chosen_segment = seg3_pred
            confidence = seg3_confidence
        else:
            model_keys = [
                f'rf_0_2seg_{data_suffix}',
                f'xgb_0_2seg_{data_suffix}',
                f'nn_0_2seg_{data_suffix}'
            ]
            chosen_system = f'general_{data_suffix}'
            chosen_segment = None
            confidence = max(seg4_confidence, seg3_confidence)
        
        available_model_keys = []
        for key in model_keys:
            if key in self.models:
                model_r2 = self.model_metrics.get(key, {'R¬≤': 0.7})['R¬≤']
                if model_r2 < 0.0:
                    print(f"–ú–æ–¥–µ–ª—å {key} –∏–º–µ–µ—Ç R¬≤ = {model_r2:.4f} < 0.0, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º...")
                    continue
                available_model_keys.append(key)
        
        if not available_model_keys:
            raise ValueError("–ù–µ –Ω–∞–π–¥–µ–Ω–æ –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è.")
        
        ensemble_prediction, model_weights = self.predict_ensemble(X_scaled, available_model_keys)
        if ensemble_prediction is None:
            raise ValueError("–ù–µ —É–¥–∞–ª–æ—Å—å –≤—ã–ø–æ–ª–Ω–∏—Ç—å –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –∞–Ω—Å–∞–º–±–ª–µ–º.")
        
        prediction_details = {}
        for model_key in available_model_keys:
            model = self.models[model_key]
            if isinstance(model, nn.Module):
                y_scaler_key = f'y_scaler_{model_key}'
                scalers_dict = self.scalers.get(y_scaler_key, None)
                if scalers_dict is None:
                    continue
                pred_array = self.predict_neural_network(model, X_scaled, scalers_dict)
                if pred_array is None or len(pred_array) == 0:
                    continue
                pred = float(pred_array[0])
            else:
                pred = model.predict(X_scaled)[0]
            
            model_type = 'nn' if 'nn' in model_key else 'rf' if 'rf' in model_key else 'xgb'
            prediction_details[model_type] = float(pred)
        
        mae_values = []
        for key in available_model_keys:
            if key in self.model_metrics:
                mae_values.append(self.model_metrics[key]['MAE'])
        avg_mae = np.mean(mae_values) if mae_values else 0.0
        scaled_mae = avg_mae * 0.5
        lower_bound = max(0, ensemble_prediction - scaled_mae)
        upper_bound = ensemble_prediction + scaled_mae
        
        best_model_key = None
        best_r2 = -float('inf')
        for model_key in available_model_keys:
            if model_key in self.model_metrics and '_nn' not in model_key:
                model_r2 = self.model_metrics[model_key]['R¬≤']
                if model_r2 > best_r2:
                    best_r2 = model_r2
                    best_model_key = model_key
        
        shap_values = self.compute_shap(self.models[best_model_key], X_scaled, self.feature_names) if best_model_key and '_nn' not in best_model_key else {}
        lime_values = self.compute_lime(X[0], self.feature_names, best_model_key, data_suffix) if best_model_key else {}
        
        self.diagnose_models()
        
        model_errors = {}
        for model_key in available_model_keys:
            if model_key in self.model_metrics:
                model_type = 'nn' if 'nn' in model_key else 'rf' if 'rf' in model_key else 'xgb'
                model_errors[model_type] = self.model_metrics[model_key].get('MAE', 0)
        
        return {
            'predicted_price_range': f"{lower_bound:,.0f} - {upper_bound:,.0f}",
            'ensemble_prediction': float(ensemble_prediction),
            'lower_bound': float(lower_bound),
            'upper_bound': float(upper_bound),
            'midpoint': float((lower_bound + upper_bound) / 2),
            'chosen_system': chosen_system,
            'chosen_segment': chosen_segment,
            'confidence': confidence,
            'prediction_details': prediction_details,
            'seg4_predictions': {'segment': seg4_pred, 'probabilities': {i: float(p) for i, p in enumerate(seg4_proba)}},
            'seg3_predictions': {'segment': seg3_pred, 'probabilities': {i: float(p) for i, p in enumerate(seg3_proba)}},
            'explanations': {
                'shap': shap_values,
                'lime': lime_values
            },
            'model_errors': model_errors,
            'model_weights': model_weights
        }
        
    def analyze_with_shap(self, model, X_scaled, feature_names):
        """SHAP –∞–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
        try:
            explainer = shap.TreeExplainer(model)
            shap_values = explainer.shap_values(X_scaled)
            
            print("\n=== SHAP –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===")
            
            if len(X_scaled) == 1:
                shap_values_single = shap_values[0]
                feature_importance = list(zip(feature_names, shap_values_single))
                feature_importance.sort(key=lambda x: abs(x[1]), reverse=True)
                
                print("\n–í–ª–∏—è–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (–ø–æ —É–±—ã–≤–∞–Ω–∏—é –≤–∞–∂–Ω–æ—Å—Ç–∏):")
                for feature, importance in feature_importance[:10]:
                    direction = "—É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç" if importance > 0 else "—É–º–µ–Ω—å—à–∞–µ—Ç"
                    print(f"{feature}: {importance:+,.0f} —Ä—É–±. ({direction} —Ü–µ–Ω—É)")
                
                base_value = float(explainer.expected_value)
                predicted_value = base_value + sum(shap_values_single)
                print(f"\n–ë–∞–∑–æ–≤–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏: {base_value:,.0f} —Ä—É–±.")
                print(f"–ò—Ç–æ–≥–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ: {predicted_value:,.0f} —Ä—É–±.")
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ SHAP –∞–Ω–∞–ª–∏–∑–∞: {e}")
    
    def analyze_with_lime(self, X_sample, feature_names, model_key, data_suffix):
        """LIME –∞–Ω–∞–ª–∏–∑ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è"""
        try:
            # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è LIME
            scaler_key = f'main_{data_suffix}'
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ª—É—á–∞–π–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Ç–æ–º –∂–µ –¥–∏–∞–ø–∞–∑–æ–Ω–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è LIME
            np.random.seed(42)
            n_samples = 1000
            training_data = []
            
            for _ in range(n_samples):
                sample = X_sample.copy()
                # –î–æ–±–∞–≤–ª—è–µ–º —à—É–º –∫ –ø—Ä–∏–∑–Ω–∞–∫–∞–º
                for i in range(len(sample)):
                    noise = np.random.normal(0, abs(sample[i]) * 0.1)
                    sample[i] += noise
                training_data.append(sample)
            
            training_data = np.array(training_data)
            
            # –°–æ–∑–¥–∞–µ–º LIME explainer
            explainer = lime.lime_tabular.LimeTabularExplainer(
                training_data,
                feature_names=feature_names,
                mode='regression',
                verbose=False
            )
            
            # –§—É–Ω–∫—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è LIME
            def predict_fn(X):
                if '_nn' in model_key:
                    y_scaler_key = f'y_scaler_{model_key.split("_rf")[0].split("_xgb")[0].split("_nn")[0]}_nn'
                    scalers_dict = self.scalers.get(y_scaler_key, None)
                    if scalers_dict is None:
                        raise ValueError(f"–ú–∞—Å—à—Ç–∞–±–∞—Ç–æ—Ä –¥–ª—è {model_key} –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç")
                    return self.predict_neural_network(self.models[model_key], X, scalers_dict)
                else:
                    return self.models[model_key].predict(X)
            
            # –ü–æ–ª—É—á–∞–µ–º –æ–±—ä—è—Å–Ω–µ–Ω–∏–µ
            explanation = explainer.explain_instance(
                X_sample, 
                predict_fn, 
                num_features=10
            )
            
            print("\n=== LIME –∞–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ===")
            print("\n–í–ª–∏—è–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ (LIME):")
            
            for feature, importance in explanation.as_list():
                direction = "—É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç" if importance > 0 else "—É–º–µ–Ω—å—à–∞–µ—Ç"
                print(f"{feature}: {importance:+,.0f} —Ä—É–±. ({direction} —Ü–µ–Ω—É)")
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–∏ LIME –∞–Ω–∞–ª–∏–∑–∞: {e}")
    def compute_shap(self, model, X_scaled, feature_names):
        try:
            explainer = shap.TreeExplainer(model)
            shap_values = explainer.shap_values(X_scaled)
            
            if len(X_scaled) == 1:
                shap_values_single = shap_values[0]
                shap_dict = {feature: float(value) for feature, value in zip(feature_names, shap_values_single)}
                return shap_dict
            return {}
        except Exception as e:
            print(f"Error in SHAP computation: {e}")
            return {}

    def compute_lime(self, X_sample, feature_names, model_key, data_suffix):
        try:
            scaler_key = f'main_{data_suffix}'
            training_data = np.random.normal(X_sample, abs(X_sample) * 0.1, size=(1000, len(X_sample)))
            
            explainer = lime.lime_tabular.LimeTabularExplainer(
                training_data,
                feature_names=feature_names,
                mode='regression',
                verbose=False
            )
            
            def predict_fn(X):
                if '_nn' in model_key:
                    y_scaler_key = f'y_scaler_{model_key}'
                    scalers_dict = self.scalers.get(y_scaler_key, None)
                    if scalers_dict is None:
                        raise ValueError(f"Y-scaler for {model_key} missing")
                    return self.predict_neural_network(self.models[model_key], X, scalers_dict)
                return self.models[model_key].predict(X)
            
            explanation = explainer.explain_instance(X_sample, predict_fn, num_features=10)
            lime_dict = {feature: float(value) for feature, value in explanation.as_list()}
            return lime_dict
        except Exception as e:
            print(f"Error in LIME computation: {e}")
            return {}
    
    def get_model_info(self):
        """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª—è—Ö"""
        if not self.models:
            return "–ú–æ–¥–µ–ª–∏ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã"
        
        info = f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(self.models)}\n"
        info += f"–ü—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(self.feature_names)}\n"
        
        if self.price_quantiles_4 is not None:
            info += f"–ö–≤–∞—Ä—Ç–∏–ª–∏: {[f'{q:,.0f}' for q in self.price_quantiles_4]}\n"
        if self.price_quantiles_3 is not None:
            info += f"–¢—Ä–µ—Ç–∏: {[f'{q:,.0f}' for q in self.price_quantiles_3]}\n"
        
        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º –º–æ–¥–µ–ª–∏ –ø–æ —Ç–∏–ø–∞–º
        model_types = {'clean': 0, 'raw': 0}
        for model_name in self.models.keys():
            if '_clean' in model_name:
                model_types['clean'] += 1
            elif '_raw' in model_name:
                model_types['raw'] += 1
        
        info += f"–ú–æ–¥–µ–ª–∏ –Ω–∞ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {model_types['clean']}\n"
        info += f"–ú–æ–¥–µ–ª–∏ –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö: {model_types['raw']}\n"
        
        return info
# –§—Ä–æ–Ω—Ç–µ–Ω–¥ –≥–æ–ª–æ–≤–Ω–æ–≥–æ –º–æ–∑–≥–∞
if __name__ == "__main__":
    import pandas as pd
    import numpy as np
    from sklearn.preprocessing import LabelEncoder, StandardScaler
    from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, r2_score, mean_squared_error, mean_absolute_error
    from xgboost import XGBRegressor
    import pickle
    import os

    # –°–æ–∑–¥–∞—ë–º —ç–∫–∑–µ–º–ø–ª—è—Ä –∫–ª–∞—Å—Å–∞ RealEstateAnalyzer
    analyzer = RealEstateAnalyzer()

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    df = pd.read_csv(r'C:\Users\nikit\Desktop\–ü—Ä–æ–µ–∫—Ç (25 —Å–µ–Ω—Ç—è–±—Ä—è)\newdata.csv', encoding='utf-8')
    df.columns = df.columns.str.strip().str.lower()

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    print("üîç –°—Ç–æ–ª–±—Ü—ã –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ:", df.columns.tolist())
    print("üîç –ò—Å—Ö–æ–¥–Ω—ã–µ —Ü–µ–Ω—ã:")
    print(df['price'].describe())
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ü–µ–Ω: {df['price'].nunique()}")
    print(f"–¢–æ–ø-5 —á–∞—Å—Ç—ã—Ö —Ü–µ–Ω:\n{df['price'].value_counts().head()}")
    print(f"–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è renovation: {df['renovation'].unique().tolist()}")

    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–ª–æ–Ω–æ–∫
    print("–ö–æ–ª–æ–Ω–∫–∏ –≤ –¥–∞—Ç–∞—Å–µ—Ç–µ –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏:", df.columns.tolist())
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    print("–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:")
    print(f"–§–æ—Ä–º–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}")
    print(f"–ö–æ–ª–æ–Ω–∫–∏: {df.columns.tolist()}")
    print("\n–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:")
    print(df.head())
    
    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –Ω—É–∂–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏
    required_columns = [
        'price', 'apartment type', 'minutes to metro', 'number of rooms', 
        'area', 'living area', 'kitchen area', 'floor', 'number of floors',
        'renovation', 'metro_lat', 'metro_lon'
    ]
    
    df = df[required_columns]
    print("\n–ü–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∫–æ–ª–æ–Ω–æ–∫:")
    print(f"–§–æ—Ä–º–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞: {df.shape}")
    print(f"–ö–æ–ª–æ–Ω–∫–∏: {df.columns.tolist()}")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö
    print("\n–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è apartment type:", df['apartment type'].unique().tolist())
    print("–£–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è renovation:", df['renovation'].unique().tolist())
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏
    print("\n–ü—Ä–æ–ø—É—Å–∫–∏ –≤ –¥–∞–Ω–Ω—ã—Ö:")
    print(df.isnull().sum())
    
    # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏ –≤ price
    initial_count = len(df)
    df = df.dropna(subset=['price'])
    print(f"\n–ü–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è —Å—Ç—Ä–æ–∫ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏ –≤ price: {df.shape}")
    
    # –°–Ω–∞—á–∞–ª–∞ –ø—ã—Ç–∞–µ–º—Å—è –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –º–æ–¥–µ–ª–∏
    loaded = analyzer.load_models()

    if loaded and len(analyzer.models) > 0:
        print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(analyzer.models)} –º–æ–¥–µ–ª–µ–π")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–µ–≤—ã–µ –º–æ–¥–µ–ª–∏
        has_clean_classifier = any('classifier_4seg_clean' in key or 'classifier_3seg_clean' in key 
                                for key in analyzer.models.keys())
        has_raw_classifier = any('classifier_4seg_raw' in key or 'classifier_3seg_raw' in key 
                                for key in analyzer.models.keys())
        
        if has_clean_classifier and has_raw_classifier:
            print("‚úÖ –í—Å–µ –∫–ª—é—á–µ–≤—ã–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã –Ω–∞–π–¥–µ–Ω—ã")
            analyzer.diagnose_models()
            models_missing = False
        else:
            print("‚ö†Ô∏è –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫–ª—é—á–µ–≤—ã–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä—ã")
            models_missing = True
    else:
        models_missing = True

    if models_missing:
        print("\nüöÄ –¢—Ä–µ–±—É–µ—Ç—Å—è –æ–±—É—á–µ–Ω–∏–µ...")
        print("\n=== –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ===")
        analyzer.train_models(df, use_cleaned_data=True)
        print("\n=== –û–±—É—á–µ–Ω–∏–µ –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ===")
        analyzer.train_models(df, use_cleaned_data=False)
        analyzer.save_models()
        print(f"\n‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ! –í—Å–µ–≥–æ –º–æ–¥–µ–ª–µ–π: {len(analyzer.models)}")
    
    # –ê–Ω–∞–ª–∏–∑ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ü–µ–Ω –¥–ª—è —Å–µ–≥–º–µ–Ω—Ç–æ–≤ (–ø–æ—Å–ª–µ –æ–±—É—á–µ–Ω–∏—è, —á—Ç–æ–±—ã –∫–≤–∞–Ω—Ç–∏–ª–∏ –±—ã–ª–∏ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã)
    print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–Ω –¥–ª—è 4seg_clean —Å–µ–≥–º–µ–Ω—Ç–∞ 3:")
    if hasattr(analyzer, 'price_quantiles_4') and analyzer.price_quantiles_4 is not None:
        print(df[df['price'] > analyzer.price_quantiles_4[2]]['price'].describe())
    else:
        print("–ö–≤–∞–Ω—Ç–∏–ª–∏ 4seg_clean –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
    print("\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ü–µ–Ω –¥–ª—è 3seg_raw —Å–µ–≥–º–µ–Ω—Ç–∞ 2:")
    if hasattr(analyzer, 'price_quantiles_3') and analyzer.price_quantiles_3 is not None:
        print(df[df['price'] > analyzer.price_quantiles_3[1]]['price'].describe())
    else:
        print("–ö–≤–∞–Ω—Ç–∏–ª–∏ 3seg_raw –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏
    analyzer.print_metrics_table()

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –º–æ–¥–µ–ª—è—Ö
    print("\n" + "="*50)
    print("–ò–ù–§–û–†–ú–ê–¶–ò–Ø –û –ú–û–î–ï–õ–Ø–•")
    print("="*50)
    print(analyzer.get_model_info())
    
    # –ú–∞—Å—à—Ç–∞–±–∞—Ç–æ—Ä –¥–ª—è general_..._nn –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç? –ù—É –∫–∞–∫ –≤—Å–µ–≥–¥–∞...
    print("\n–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–∞—Å—à—Ç–∞–±–∞—Ç–æ—Ä—ã:", list(analyzer.scalers.keys()))
    print("–ú–µ—Ç—Ä–∏–∫–∏ –º–æ–¥–µ–ª–µ–π:", analyzer.model_metrics)
    
    # –ü—Ä–∏–º–µ—Ä –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ –Ω–∞–∑–≤–∞–Ω–∏—è–º–∏ –ø–æ–ª–µ–π
    print("\n" + "="*50)
    print("–ü–†–ò–ú–ï–† –ü–†–ï–î–°–ö–ê–ó–ê–ù–ò–Ø")
    print("="*50)
    
    sample_data = {
        'apartment type': 'secondary',
        'minutes to metro': 5.0,
        'number of rooms': 1,
        'area': 42.0,
        'living area': 32.0,
        'kitchen area': 10.0,
        'floor': 5,
        'number of floors': 9,
        'renovation': 'cosmetic',
        'metro_lat': 55.7558,
        'metro_lon': 37.6173
    }
    
    print("\n--- –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –º–æ–¥–µ–ª—è–º–∏ –Ω–∞ –æ—á–∏—â–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ---")
    try:
        prediction = analyzer.predict_price(sample_data, use_cleaned_models=True)
        print("\n–ò—Ç–æ–≥–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:", prediction['predicted_price_range'], "—Ä—É–±.")
        print("–î–µ—Ç–∞–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:", prediction)
    except ValueError as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {e}")
    
    print("\n--- –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ —Å –º–æ–¥–µ–ª—è–º–∏ –Ω–∞ –∏—Å—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö ---")
    try:
        prediction = analyzer.predict_price(sample_data, use_cleaned_models=False)
        print("\n–ò—Ç–æ–≥–æ–≤–æ–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ:", prediction['predicted_price_range'], "—Ä—É–±.")
        print("–î–µ—Ç–∞–ª–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è:", prediction)
    except ValueError as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–∏: {e}")